# ğŸš¢ Dockerized Deployment â€“ Supply Chain Threat Detector

This project uses Docker to containerize both the **FastAPI backend** and **Streamlit frontend**.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ backend_api.py
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ rebuild_docker.bat
â””â”€â”€ README_DOCKER.md
```

---

## ğŸš€ Quick Start (Local Docker Build)

### ğŸ” Rebuild from Scratch
Run the `.bat` script (for Windows users):

```bash
./rebuild_docker.bat
```

This will:
1. Stop existing containers
2. Rebuild images using `docker-compose`
3. Start both backend and frontend services

---

## ğŸ§ª Manual Docker Commands

### Build & Run via Docker Compose

```bash
docker-compose down
docker-compose up --build
```

---

## ğŸŒ Accessing the App

- Frontend (Streamlit): [http://localhost:8501](http://localhost:8501)
- Backend (FastAPI): [http://localhost:8080/docs](http://localhost:8080/docs)

---

## âš™ï¸ Environment & Model Setup

### Models
To use local models like TinyLLaMA, ensure your `models/` folder is **excluded** from `.dockerignore` and structured like:

```
models/
â””â”€â”€ tinylama-1.1b-chat-v1.0.Q4_K_M.gguf
```

---

## ğŸ“¦ Notes

- `llama-cpp-python` and `faiss-cpu` are installed and pinned in `requirements.txt`.
- Dependencies are resolved using pip with `--no-cache-dir` to keep builds lean.
- Compatibility ensured for `langchain==0.1.17` and `langchain-community==0.0.36`.

---

## ğŸ§¼ .dockerignore Example

```
__pycache__/
*.pyc
*.pyo
*.pyd
*.db
.env
models/
```

---

## ğŸ›  Troubleshooting

- â— If the backend fails to find a model path, verify `models/` exists inside the container (`docker exec -it container_name ls /app/models`)
- âœ… Ensure `llama-cpp-python` is installed and compatible with your system's CPU

